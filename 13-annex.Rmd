# Technical Annex {-}

This chapter contains technical details on the algorithms available in `sits`. It is intended to support those that want to understand how the package works and also want to contribute to its development.

##  Bayesian smoothing {-}

Doing post-processing using Bayesian smoothing in SITS is straightforward. The result of the `sits_classify` function applied to a data cube is set of probability images, one per class. The next step is to apply the `sits_smooth` function. By default, this function selects the most likely class for each pixel considering only the probabilities of each class for each pixel. For continuous probability distributions, Bayesian inference is expressed by the rule:

$$
\pi(\theta|x) \propto \pi(x|\theta)\pi(\theta)
$$

Bayesian inference involves the estimation of an unknown parameter $\theta$, which is the random variable that describe what we are trying to measure. In the case of smoothing of image classification, $\theta$ is the class probability for a given pixel, conditioned by the probability values of that pixel. We model our initial belief about this value by a probability distribution,  $\pi(\theta)$, called the \emph{prior} distribution. It represents what we know about $\theta$ \emph{before} observing the data. The distribution $\pi(x|\theta)$, called the \emph{likelihood}, is estimated based on the observed data. It represents the added information provided by our observations. The \emph{posterior} distribution $\pi(\theta|x)$ is our improved belief of $\theta$ \emph{after} seeing the data. Bayes's rule states that the \emph{posterior} probability is proportional to the product of the \emph{likelihood} and the \emph{prior} probability.

### Derivation of bayesian parameters for spatiotemporal smoothing{-}

In our post-classification smoothing model, we consider the output of a machine learning algorithm that provides the probabilities of each pixel in the image to belong to target classes. More formally, consider a set of $K$ classes that are candidates for labelling each pixel. Let $p_{i,t,k}$ be the probability of pixel $i$ belonging to class $k$, $k = 1, \dots, K$. We have 
$$
\sum_{k=1}^K p_{i,k} = 1, p_{i,k} > 0
$$
We label a pixel $p_i$ as being of class $k$ if
$$
	p_{i, k} > p_{i,m}, \forall m = 1, \dots, K, m \neq k
$$


For each pixel $i$, we take the odds of the classification for class $k$, expressed as
$$
	O_{i,k} = p_{i,k} / (1-p_{i,k})
$$
where $p_{i, k}$ is the probability of class $k$. We have more confidence in pixels with higher odds since their class assignment is stronger. There are situations, such as border pixels or mixed ones, where the odds of different classes are similar in magnitude. We take them as cases of low confidence in the classification result. To assess and correct these cases,  Bayesian smoothing methods borrow strength from the neighbors and reduces the variance of the estimated class for each pixel.

We further make the transformation 
$$
	x_{i,k} = \log [O_{i,k}]
$$
which measures the *logit* (log of the odds) associated to classifying the pixel $i$ as being of class $k$. The support of $x_{i, k}$ is $\mathbb{R}$. We can express the pixel data as a $K$-dimensional multivariate logit vector 

$$
\mathbf{x}_{i}=(x_{i,k_{0}},x_{i,k_{1}},\dots{},x_{i,k_{K}})
$$ 


For each pixel, the random variable that describes the class probability $k$ is denoted by $\theta_{i,k}$. This formulation allows uses to use the class covariance matrix in our formulations. We can express Bayes' rule for all combinations of pixel and classes for a time interval as

$$
\pi(\boldsymbol\theta_{i}|\mathbf{x}_{i}) \propto \pi(\mathbf{x}_{i}|\boldsymbol\theta_{i})\pi(\boldsymbol\theta_{i}).	
$$

We assume the conditional distribution $\mathbf{x}_{i}|\boldsymbol\theta_{i}$ follows a multivariate normal distribution

$$
    [\mathbf{x}_{i}|\boldsymbol\theta_{i}]\sim\mathcal{N}_{K}(\boldsymbol\theta_{i},\boldsymbol\Sigma_{i}),
$$

where $\boldsymbol\theta_{i}$ is the mean parameter vector for the pixel $i$, and $\boldsymbol\Sigma_{i}$ is a known $k\times{}k$ covariance matrix that we will use as a parameter to control the level of smoothness effect. We will discuss later on how to estimate $\boldsymbol\Sigma_{i}$. To model our uncertainty about the parameter $\boldsymbol\theta_{i}$, we will assume it also follows a multivariate normal distribution with hyper-parameters $\mathbf{m}_{i}$ for the mean vector, and $\mathbf{S}_{i}$ for the covariance matrix. 

$$
    [\boldsymbol\theta_{i}]\sim\mathcal{N}_{K}(\mathbf{m}_{i}, \mathbf{S}_{i}).
$$

The above equation defines our prior distribution. The hyper-parameters $\mathbf{m}_{i}$ and $\mathbf{S}_{i}$ are obtained using the neighboring pixels of pixel $i$. The neighborhood can be defined as any graph scheme (e.g. a given Chebyshev distance on the time-space lattice) and can include the referencing pixel $i$ as a neighbor. More formally, let 

$$
    \mathbf{V}_{i}=\{\mathbf{x}_{i_{j}}\}_{j=1}^{N}, 
$$
denote the $N$ logit vectors of a spatiotemporal neighborhood $N$ of pixel $i$. Then the prior mean is calculated by

$$
	\mathbf{m}_{i}=\operatorname{E}[\mathbf{V}_{i}],
$$

and the prior covariance matrix by

$$
    \mathbf{S}_{i}=\operatorname{E}\left[
      \left(\mathbf{V}_{i}-\mathbf{m}_{i}\right)
      \left(\mathbf{V}_{i}-\mathbf{m}_{i}\right)^\intercal
    \right].
$$
The $\boldsymbol\theta_{i}$ parameter model is our initial belief about a pixel vector using the neighborhood information in the prior distribution. It represents what we know about the probable value of $\mathbf{x}_{i}$ (and hence, about the class probabilities as the logit function is a monotonically increasing function) \emph{before} observing it. The \emph{likelihood} function $P[\mathbf{x}_{i,t}|\boldsymbol\theta_{i,t}]$ represents the added information provided by our observation of $\mathbf{x}_{i,t}$. The \emph{posterior} probability density function $P[\boldsymbol\theta_{i,t}|\mathbf{x}_{i,t}]$ is our improved belief of the pixel vector \emph{after} seeing $\mathbf{x}_{i,t}$.

Since the likelihood and prior are multivariate normal distributions, the posterior will also be a multivariate normal distribution, whose updated parameters can be derived by applying the density functions associated to the above equations. The posterior distribution is given by

$$
    [\boldsymbol\theta_{i}|\mathbf{x}_{i}]\sim\mathcal{N}_{K}\left(
    (\mathbf{S}_{i}^{-1} + \boldsymbol\Sigma^{-1})^{-1}( \mathbf{S}_{i}^{-1}\mathbf{m}_{i} + \boldsymbol\Sigma^{-1} \mathbf{x}_{i}),
    (\mathbf{S}_{i}^{-1} + \boldsymbol\Sigma^{-1})^{-1}
    \right).
$$
At this point, we are able to infer the estimator $\hat{\boldsymbol\theta}_{i}$ for the $\boldsymbol\theta_{i}|\mathbf{x}_{i}$ parameter. For the multivariate normal distribution, the posterior mean minimizes the quadratic loss but the absolute and zero-one loss functions. It can be taken from the updated mean parameter of the posterior distribution which, after some algebra, can be expressed as

$$
    \hat{\boldsymbol{\theta}}_{i}=\operatorname{E}[\boldsymbol\theta_{i}|\mathbf{x}_{i}]=\boldsymbol\Sigma_{i}\left(\boldsymbol\Sigma_{i}+\mathbf{S}_{i}\right)^{-1}\mathbf{m}_{i} +
    \mathbf{S}_{i}\left(\boldsymbol\Sigma_{i}+\mathbf{S}_{i}\right)^{-1}\mathbf{x}_{i}.
$$

The estimator value for the logit vector $\hat{\boldsymbol\theta}_{i}$ is a weighted average of the original logit vector $\mathbf{x}_{i}$ and the neighborhood mean vector $\mathbf{m}_{i}$. The weights are given by the covariance matrix $\mathbf{S}_{i}$ of the prior distribution and the covariance matrix of the conditional distribution. The matrix $\mathbf{S}_{i}$ is calculated considering the  neighbors and the matrix $\boldsymbol\Sigma_{i}$ is the smoothing factor provided as prior belief by the user. 

When the values of local class covariance $\mathbf{S}_{i}$ are higher than those the conditional covariance $\boldsymbol\Sigma_{i}$, our confidence on the influence of the neighbors is low, and the smoothing algorithm gives more weight to the original pixel value $x_{i,k}$. When the local class covariance $\mathbf{S}_{i}$ decreases relative to the smoothness factor $\boldsymbol\Sigma_{i}$, our confidence on the influence of the neighborhood increases. The smoothing procedure will be most relevant in situations where the original classification odds ratio is low, showing a low level of separability between classes. In these cases, the updated values of the classes will be influenced by the local class variances. 

In practice, $\boldsymbol\Sigma_{i}$ is a user-controlled covariance matrix parameter that will be set by users based on their knowledge of the region to be classified. In the simplest case, users can associate the  conditional covariance $\boldsymbol\Sigma_{i}$ to a diagonal matrix, using only one hyperparameter $\sigma^2_k$ to set the level of smoothness. Higher values of $\sigma^2_k$ will cause the assignment of the local mean to the pixel updated probability. In our case, after some classification tests, we decided to use $\sigma^2_k=20$ by default for all $k$. 

The version implemented in `sits` includes the following parameters

- `smoothness`: user-controlled parameter that controls the influence of the neighborhood values on the probabibility value of each class of a pixel. Can be defined as a unique value for all classes, or a matrix whose size of size $num_classes * num_classes$. The default value is 20, but users are encouraged define `smoothness` as a diagonal matrix whose values reflect the relative importance of each class in the output product. 
- `window_size`: size of $n*n$ window used to define the neighborhood.
- `neigh_fraction`: percentage of pixels per window used to calculate local class statistics. The aim is to use only those pixels in the window that are likely to be part of the same class as the central pixel. For example, for a `window_size` of size 9 and `neigh_fraction` of 0.5, half of the pixels of the 9 x 9 window (those with higher class probabibility) will be used to estimate the local class statistics $\mathbf{m}_{i,t}$ and $\mathbf{S}_{i,t}$ which represent the mean and standard deviation of the classes associated to prox

## Bilateral smoothing {-}

Bilateral smoothing combines proximity (combining pixels which are close) and similarity (comparing the values of the pixels) [@Tomasi1998]. If most of the pixels in a neighborhood have similar values, it is easy to identify outliers and noisy pixels. In contrast, there is a strong difference between the values of pixels in a if neighborhood, it is possible that the pixel is located in a class boundary. Bilateral filtering combines domain filtering with range filtering. In domain filtering, the weights used to combine pixels decrease with distance. In range filtering, the weights are computed considering value similarity. 

The combination of domain and range filtering is mathematically expressed as: 

$$
S(x_i) = \frac{1}{W_{i}} \sum_{x_k \in \theta} I(x_k)\mathcal{N}_{\tau}(\|I(x_k) - I(x_i)\|)\mathcal{N}_{\sigma}(\|x_k - x_i\|),
$$
where

- $S(x_i)$  is the smoothed value of pixel $i$;
- $I$ is the original probability image to be filtered;
- $I(x_i)$ is the value of pixel $i$;
- $\theta$ is the neighborhood centered in $x_i$;
- $x_k$ is a pixel $k$ which belongs to neighborhood $\theta$;
- $I(x_k)$ is the value of a pixel $k$ in the neighborhood of pixel $i$;
- $\|I(x_k) - I(x_i)\|$ is the absolute difference between the values of the pixel $k$ and pixel $i$;
- $\|x_k - x_i\|$ is the distance between pixel $k$ and pixel $i$;
- $\mathcal{N}_{\tau}$ is the Gaussian range kernel for smoothing differences in intensities;
- $\mathcal{N}_{\sigma}$is the Gaussian spatial kernel for smoothing differences based on proximity.
- $\tau$ is the variance of the Gaussian range kernel;
- $\sigma$ is the variance of the Gaussian spatial kernel.

The normalization term to be applied to compute the smoothed values of pixel $i$ is defined as

$$
W_{i} = \sum_{x_k \in \theta}{\mathcal{N}_{\tau}(\|I(x_k) - I(x_i)\|)\mathcal{N}_{\sigma}(\|x_k - x_i\|)}
$$
## How parallel processing works{-}

This section provides an overview of how the functions `sits_classify()`, `sits_smooth()` and `sits_label_classification()` process images in parallel. To achieve efficiency, `sits` implements a fault tolerant multitasking procedure for big EO data classification. Users are not burdened with the need to learn how to do multiprocessing. Thus, their learning curve is shortened. Image classification in `sits` is done by a cluster of independent workers linked to a virtual machine. To avoid communication overhead, all large payloads are read and stored independently; direct interaction between the main process and the workers is kept at a minimum. 

The classification procedure benefits from the fact that most images available in cloud collections are stored as COGs (cloud-optimized Geotiff). COGs are a regular GeoTIFF files organized in regular square blocks to improve visualization and access for large data sets. Thus, data requests can be optimized to access only portions of the images. All cloud services supported by `sits` use COG files. The classification algorithm in `sits` uses COGs to ensure optimal data access, reducing I/O demand as much as possible.

The approach for parallel processing in `sits`, depicted in the figure below, has the following steps:

1. Based on the block size of individual COG files, calculate the size of each chunk that has to be loaded in memory, considering the number of bands and the length of the timeline. Chunk access is optimized for efficient transfer of data blocks.
2. Divide the total memory available by the chunk size to find out how many processes can be run in parallel. 
3. Each core processes a chunk and produces a subset of the result.
4. Repeat the process until all chunks in the cube have been processed.
5. Check that subimages have been produced correctly. If there is a problem with one or more subimages, run a failure recovery procedure to ensure all data is processed.
6. After all subimages are generated, join them to obtain the result.

```{r, out.width = "90%", out.height = "90%", echo = FALSE, fig.align="center", fig.cap="Parallel processing in sits (source: Simoes et al.,2021)."}

knitr::include_graphics("images/sits_parallel.png")
```

This approach has many advantages. It works in any virtual machine that supports R and has no dependencies on proprietary software. Processing is done in a concurrent and independent way, with no communication between workers. Failure of one worker does not cause failure of the big data processing. The software is prepared to resume classification processing from the last processed chunk, preventing against failures such as memory exhaustion, power supply interruption, or network breakdown. 

To reduce processing time, it is necessary to adjust `sits_classify()`, `sits_smooth()`, and `sits_label_classification()`  according to the capabilities of the host environment. The `memsize` parameter controls the size of the main memory (in GBytes) to be used for classification. A practical approach is to set `memsize` to the maximum memory available in the virtual machine for classification and to chose `multicores` as the largest number of cores available. Based on the memory available and the size of blocks in COG files, `sits` will access the images in an optimized way. In this way, `sits` tries to ensure best possible use of the available resources. 